{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "881b6709",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path \n",
    "\n",
    "\n",
    "class ProjectPaths:\n",
    "    \"\"\"Centralized path management for the flood detection project\"\"\"\n",
    "    \n",
    "    def __init__(self, project_dir: Path):\n",
    "        self.project_dir = project_dir\n",
    "        # self.image_code = image_code\n",
    "        \n",
    "        \n",
    "        # Main working directories\n",
    "        self.dataset_dir = self.project_dir / \"data\" /  \"4final\" / \"dataset\"\n",
    "        self.training_dir = self.project_dir / \"data\" / \"4final\" / \"training\"\n",
    "        self.predictions_dir = self.project_dir / \"data\" / \"4final\" / 'predictions'\n",
    "        \n",
    "        # Data subdirectories\n",
    "        self.images_dir = self.dataset_dir / 'S1Hand'\n",
    "        self.labels_dir = self.dataset_dir / 'LabelHand'\n",
    "        \n",
    "        # CSV files\n",
    "        self.train_csv = self.dataset_dir / \"flood_train_data.csv\"\n",
    "        self.val_csv = self.dataset_dir / \"flood_valid_data.csv\"\n",
    "        self.test_csv = self.dataset_dir / \"flood_test_data.csv\"\n",
    "        \n",
    "        # Checkpoint directories (consolidated)\n",
    "        self.ckpt_input_dir = project_dir / \"checkpoints\" / 'ckpt_INPUT'\n",
    "        self.ckpt_training_dir = self.project_dir / \"checkpoints\" / \"ckpt_training\"\n",
    "        \n",
    "        # Config files\n",
    "        self.main_config = project_dir / \"configs\" / \"floodaiv2_config.yaml\"\n",
    "        self.minmax_config = project_dir / \"configs\" / \"global_minmax_INPUT\" / \"global_minmax.json\"\n",
    "        \n",
    "        # Environment file\n",
    "        self.env_file = self.project_dir / \".env\"\n",
    "    \n",
    "    def get_inference_paths(self, sensor: str = 'sensor', date: str = 'date', tile_size: int =512, threshold: float = 0.5,  image_code: str = '0000', output_filename: str = '_name') -> dict:\n",
    "        \"\"\"Get inference-specific paths\"\"\"\n",
    "        save_tiles_path = self.predictions_dir / f'{image_code}_tiles'\n",
    "        return {\n",
    "            'predict_input': self.project_dir / \"data\" / \"4final\" / \"predict_input\",\n",
    "            'pred_tiles_dir': self.predictions_dir / f'{image_code}_predictions',\n",
    "            'save_tiles_path': save_tiles_path,\n",
    "            'extracted_dir': self.predictions_dir / f'{image_code}_extracted',\n",
    "            'file_list': self.predictions_dir / \"predict_tile_list.csv\",\n",
    "            'stitched_image': self.predictions_dir / f'{sensor}_{image_code}_{date}_{tile_size}_{threshold}_{output_filename}_WATER_AI.tif',\n",
    "            'metadata_path': save_tiles_path / 'tile_metadata.json'\n",
    "        }\n",
    "    \n",
    "    def get_training_paths(self):\n",
    "        \"\"\"Get training/testing specific paths\"\"\"\n",
    "        return {\n",
    "            'save_tiles_path': self.dataset_dir,\n",
    "            'metadata_path': self.dataset_dir / 'tile_metadata_pth.json'\n",
    "        }\n",
    "    \n",
    "    def validate_paths(self, job_type: str):\n",
    "        \"\"\"Validate that required paths exist for the given job type\"\"\"\n",
    "        errors = []\n",
    "        required_paths = []\n",
    "        if job_type in ('train', 'test'):\n",
    "            required_paths = [\n",
    "                (self.dataset_dir, \"Dataset directory\"),\n",
    "                (self.images_dir, \"Images directory\"),\n",
    "                (self.labels_dir, \"Labels directory\"),\n",
    "            ]\n",
    "            \n",
    "            if job_type == 'train':\n",
    "                required_paths.extend([\n",
    "                    (self.train_csv, \"Training CSV\"),\n",
    "                    (self.val_csv, \"Validation CSV\")\n",
    "                ])\n",
    "            elif job_type == 'test':\n",
    "                required_paths.append((self.test_csv, \"Test CSV\"))\n",
    "                \n",
    "        elif job_type == 'inference':\n",
    "            # Inference paths are created dynamically, less validation needed\n",
    "            pass\n",
    "            \n",
    "        # Always check checkpoint folder\n",
    "        required_paths.append((self.ckpt_input_dir, \"Checkpoint input directory\"))\n",
    "        \n",
    "        for path, description in required_paths:\n",
    "            if not path.exists():\n",
    "                errors.append(f\"{description} not found: {path}\")\n",
    "        \n",
    "        # Check for checkpoint files\n",
    "        if not any(self.ckpt_input_dir.rglob(\"*.ckpt\")):\n",
    "            errors.append(f\"No checkpoint files found in: {self.ckpt_input_dir}\")\n",
    "            \n",
    "        return errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6b400ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/alexwebb/laptop_coding/floodai/InferSAR/data/4final/predict_input\n"
     ]
    }
   ],
   "source": [
    "project_dir = Path('/Users/alexwebb/laptop_coding/floodai/InferSAR')\n",
    "paths = ProjectPaths(project_dir=project_dir)\n",
    "\n",
    "inf_pths = paths.get_inference_paths(sensor='S1A', date='20230101', tile_size=512, threshold=0.5, image_code='0001', output_filename='test_output')\n",
    "print(inf_pths['predict_input'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "floodai_train",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
